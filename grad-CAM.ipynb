{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    if len(cnts) == 0:\n",
    "        return img  \n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    ADD_PIXELS = 5\n",
    "    y1, y2 = max(0, extTop[1] - ADD_PIXELS), min(img.shape[0], extBot[1] + ADD_PIXELS)\n",
    "    x1, x2 = max(0, extLeft[0] - ADD_PIXELS), min(img.shape[1], extRight[0] + ADD_PIXELS)\n",
    "    new_img = img[y1:y2, x1:x2].copy()\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img,img_size=224):\n",
    "            img = crop_img(img)\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            img = cv2.bilateralFilter(img, 2, 50, 50)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = np.uint8(img)\n",
    "            img = cv2.applyColorMap(img, cv2.COLORMAP_BONE)\n",
    "            return img/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Superimpose heatmap on original image\u001b[39;00m\n\u001b[0;32m    138\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m  \u001b[38;5;66;03m# Transparency factor\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m superimposed_img \u001b[38;5;241m=\u001b[39m heatmap \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m+\u001b[39m original_img_rgb \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha)\n\u001b[0;32m    140\u001b[0m superimposed_img \u001b[38;5;241m=\u001b[39m superimposed_img \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(superimposed_img)  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Visualize\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Produces class activation map using the gradient information\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        # Register the hooks\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        # Forward pass\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Target for backprop\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        \n",
    "        # Backward pass\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        # Get weights\n",
    "        gradients = self.gradients.cpu().data.numpy()[0]\n",
    "        \n",
    "        # Take the average of gradients over each channel (global average pooling)\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        \n",
    "        # Get activation maps\n",
    "        activations = self.activations.cpu().data.numpy()[0]\n",
    "        \n",
    "        # Create weighted sum of activation maps\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        # Apply ReLU\n",
    "        cam = np.maximum(cam, 0)\n",
    "        \n",
    "        # Normalize the CAM\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
    "        \n",
    "        return cam, target_class\n",
    "\n",
    "# Load and prepare image\n",
    "image_path = \"Preprocessed_Dataset\\Testing\\glioma\\Te-gl_0013.jpg\"\n",
    "img = cv2.imread(image_path)\n",
    "img = preprocess(img)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "input_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# Define model class\n",
    "class BrainTumorClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BrainTumorClassifier, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "            # Note: removed Softmax here, as it can interfere with gradient flow\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BrainTumorClassifier(num_classes=4)\n",
    "\n",
    "checkpoint = torch.load('./Model/resnet-50_finetuned.pth', map_location=device)\n",
    "\n",
    "# If your checkpoint contains 'model_state_dict' key\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Get the target layer - last convolutional layer\n",
    "target_layer = model.base_model.layer4[-1].conv3\n",
    "\n",
    "# Initialize GradCAM\n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "\n",
    "# Generate CAM\n",
    "cam, pred_idx = grad_cam.generate_cam(input_tensor)\n",
    "\n",
    "# Resize CAM to match image size\n",
    "cam_resized = cv2.resize(cam, (224, 224))\n",
    "\n",
    "# Class names\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "predicted_class = class_names[pred_idx]\n",
    "\n",
    "# Create heatmap\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert original image to RGB if it's not\n",
    "if len(original_img.shape) == 2:  # Grayscale\n",
    "    original_img_rgb = cv2.cvtColor(original_img, cv2.COLOR_GRAY2RGB)\n",
    "else:\n",
    "    original_img_rgb = original_img\n",
    "\n",
    "# Superimpose heatmap on original image\n",
    "alpha = 0.6  # Transparency factor\n",
    "superimposed_img = heatmap * alpha + original_img_rgb * (1 - alpha)\n",
    "superimposed_img = superimposed_img / np.max(superimposed_img)  # Normalize\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_img_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(cam_resized, cmap='jet')\n",
    "plt.title('Grad-CAM Heatmap')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(superimposed_img)\n",
    "plt.title(f'Superimposed: {predicted_class}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('improved_gradcam_result.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Try another visualization approach with cv2\n",
    "heatmap_img = cv2.resize(heatmap, (original_img_rgb.shape[1], original_img_rgb.shape[0]))\n",
    "cv2_superimposed = cv2.addWeighted(original_img_rgb, 0.6, heatmap_img, 0.4, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_img_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2_superimposed)\n",
    "plt.title(f'CV2 Superimposed: {predicted_class}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cv2_gradcam_result.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
